{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT OF PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import io\n",
    "import pickle\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Embedding, Input, LSTM, Bidirectional, Dropout, MaxPooling1D, Conv1D, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataTask3(folder):\n",
    "    result = []\n",
    "    fileNames = glob.glob(folder + \"/*.txt\")\n",
    "    for fileName in fileNames:\n",
    "        articleId = fileName.split(\"/\")[-1].split(\".\")[0]\n",
    "        f = open(fileName, \"r\", encoding=\"utf8\")\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "        sentences = [x for x in data.split(\"\\n\") if x != \"\"]\n",
    "        labels = readLabelTask3(folder + \"/\" + articleId + \".task3.labels\")\n",
    "        result.append({\"id\": articleId, \"data\": data, \"sentences\": sentences, \"labels\": labels})\n",
    "        \n",
    "    return result\n",
    "\n",
    "def readLabelTask3(fileName):\n",
    "    result = []\n",
    "    f = open(fileName, \"r\")\n",
    "    result = f.readlines()\n",
    "    f.close()\n",
    "    result = [x.replace(\"\\n\", \"\").split(\"\\t\") for x in result]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = loadDataTask3(\"train-split/tasks-2-3/train-train/\")\n",
    "dev_data = loadDataTask3(\"train-split/tasks-2-3/train-dev/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2index = {\n",
    "    \"Appeal_to_Authority\": 0,\n",
    "    \"Appeal_to_fear-prejudice\": 1,\n",
    "    \"Bandwagon\": 2,\n",
    "    \"Black-and-White_Fallacy\": 3,\n",
    "    \"Causal_Oversimplification\": 4,\n",
    "    \"Doubt\": 5,\n",
    "    \"Exaggeration,Minimisation\": 6,\n",
    "    \"Flag-Waving\": 7,\n",
    "    \"Loaded_Language\": 8,\n",
    "    \"Name_Calling,Labeling\": 9,\n",
    "    \"Obfuscation,Intentional_Vagueness,Confusion\": 10,\n",
    "    \"Red_Herring\": 11,\n",
    "    \"Reductio_ad_hitlerum\": 12,\n",
    "    \"Repetition\": 13,\n",
    "    \"Slogans\": 14,\n",
    "    \"Straw_Men\": 15,\n",
    "    \"Thought-terminating_Cliches\": 16,\n",
    "    \"Whataboutism\": 17\n",
    "}\n",
    "\n",
    "index2label = [\n",
    "    \"Appeal_to_Authority\",\n",
    "    \"Appeal_to_fear-prejudice\",\n",
    "    \"Bandwagon\",\n",
    "    \"Black-and-White_Fallacy\",\n",
    "    \"Causal_Oversimplification\",\n",
    "    \"Doubt\",\n",
    "    \"Exaggeration,Minimisation\",\n",
    "    \"Flag-Waving\",\n",
    "    \"Loaded_Language\",\n",
    "    \"Name_Calling,Labeling\",\n",
    "    \"Obfuscation,Intentional_Vagueness,Confusion\",\n",
    "    \"Red_Herring\",\n",
    "    \"Reductio_ad_hitlerum\",\n",
    "    \"Repetition\",\n",
    "    \"Slogans\",\n",
    "    \"Straw_Men\",\n",
    "    \"Thought-terminating_Cliches\",\n",
    "    \"Whataboutism\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROPAGANDA IDENTIFICATION MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSentence(sentence, char, start=None, stop=None):\n",
    "    if start is None:\n",
    "        start = 0;\n",
    "        \n",
    "    if stop is None:\n",
    "        stop = len(sentence)\n",
    "        \n",
    "    s = list(sentence)\n",
    "    for i in range(start, stop):\n",
    "        if i < len(s):\n",
    "            if s[i] in '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ':\n",
    "                s[i] = \" \"\n",
    "            if s[i] not in [\" \", \"\\n\"]:\n",
    "                s[i] = char\n",
    "    return \"\".join(s)\n",
    "\n",
    "def word2label(word):\n",
    "    if word[0] == \"A\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def sen2label(sen):\n",
    "    result = [word2label(x) for x in sen if x != \"\"]\n",
    "    return result\n",
    "\n",
    "def getTrainData(data):\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    for x in data:\n",
    "        trainX += x[\"sentences\"]\n",
    "        \n",
    "        dataMask = updateSentence(x[\"data\"], \"A\")\n",
    "        for l in x[\"labels\"]:\n",
    "                dataMask = updateSentence(dataMask, \"B\", int(l[2]), int(l[3]))\n",
    "        outcome = [sen2label(y.split(\" \")) for y in dataMask.split(\"\\n\") if y != \"\"]\n",
    "        trainY += outcome\n",
    "\n",
    "        \n",
    "    return trainX, trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = getTrainData(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZATION AND WORD EMBEDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenization\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(len(s) for s in tokenizer.texts_to_sequences(train_X))\n",
    "print('Max sequence length:', max_sequence_length)\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "# load in pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(os.path.join('glove.6B/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n",
    "    # is just a space-separated text file in the format:\n",
    "    # word vec[0] vec[1] vec[2] ...\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 32\n",
    "\n",
    "print('Building model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Conv1D(filters=LATENT_DIM, kernel_size=5, padding=\"same\"))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=1, padding=\"same\"))\n",
    "model.add(Conv1D(filters=LATENT_DIM, kernel_size=4, padding=\"same\"))\n",
    "model.add(MaxPooling1D(pool_size=4, strides=1, padding=\"same\"))\n",
    "model.add(Conv1D(filters=LATENT_DIM, kernel_size=3, padding=\"same\"))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=1, padding=\"same\"))\n",
    "model.add(TimeDistributed(Dense(20, activation=\"relu\")))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=Adam(lr=0.01),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = pad_sequences(tokenizer.texts_to_sequences(train_X),\n",
    "                                maxlen=max_sequence_length, padding='post')\n",
    "output_sequences = pad_sequences(train_Y, maxlen=max_sequence_length, padding='post')\n",
    "output_sequences = np.reshape(output_sequences, output_sequences.shape + (1,))\n",
    "\n",
    "print('Shape of data tensor:', input_sequences.shape)\n",
    "print('Shape of output tensor:', output_sequences.shape)\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "print('Training model...')\n",
    "z = np.zeros((len(input_sequences), LATENT_DIM))\n",
    "seed(1)\n",
    "set_random_seed(2)\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=Adam(lr=0.01),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "model.fit(\n",
    "  input_sequences,\n",
    "  output_sequences,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "pred_Y = model.predict(input_sequences).round()\n",
    "pred_Y = np.reshape(pred_Y, pred_Y.shape[:2])\n",
    "act_Y = np.reshape(output_sequences, output_sequences.shape[:2])\n",
    "\n",
    "f1 = f1_score(act_Y, pred_Y, average='micro')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_prob_cnn_v1.json', \"w\") as json_file:\n",
    "        json_file.write(model.to_json())\n",
    "model.save_weights('model_prob_cnn_v1.h5')\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_X, dev_Y = getTrainData(dev_data)\n",
    "input_sequences_dev = pad_sequences(tokenizer.texts_to_sequences(dev_X),\n",
    "                                maxlen=max_sequence_length, padding='post')\n",
    "output_sequences_dev = pad_sequences(dev_Y, maxlen=max_sequence_length, padding='post')\n",
    "output_sequences_dev = np.reshape(output_sequences_dev, output_sequences_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = model.predict(input_sequences_dev).round()\n",
    "pred_Y = np.reshape(pred_Y, pred_Y.shape[:2])\n",
    "#act_Y = np.reshape(output_sequences_dev, output_sequences.shape[:2])\n",
    "\n",
    "f1 = f1_score(output_sequences_dev, pred_Y, average='micro')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABELS MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelOutcome(label):\n",
    "    indx = label2index[label]\n",
    "    result = np.zeros(18, dtype=int)\n",
    "    result[indx] = 1\n",
    "    return result\n",
    "\n",
    "def getLabelsData(data):\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    for x in data:\n",
    "        for y in x[\"labels\"]:\n",
    "            sen = [z for z in x[\"data\"][int(y[2]):int(y[3])].split(\"\\n\") if z != \"\"]\n",
    "            lab = [labelOutcome(y[1]) for z in range(len(sen))]\n",
    "            \n",
    "            trainX += sen\n",
    "            trainY += lab\n",
    "        \n",
    "    return trainX, trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = getLabelsData(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenization\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "with open('tokenizer_label.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(len(s) for s in tokenizer.texts_to_sequences(train_X))\n",
    "print('Max sequence length:', max_sequence_length)\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 32\n",
    "print('Building model...')\n",
    "\n",
    "modelCNN = Sequential()\n",
    "modelCNN.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False))\n",
    "modelCNN.add(Conv1D(filters=LATENT_DIM, kernel_size=5, padding=\"same\"))\n",
    "modelCNN.add(MaxPooling1D(pool_size=3, strides=1, padding=\"same\"))\n",
    "modelCNN.add(Conv1D(filters=LATENT_DIM, kernel_size=4, padding=\"same\"))\n",
    "modelCNN.add(MaxPooling1D(pool_size=4, strides=1, padding=\"same\"))\n",
    "modelCNN.add(Conv1D(filters=LATENT_DIM, kernel_size=3, padding=\"same\"))\n",
    "modelCNN.add(GlobalMaxPool1D())\n",
    "modelCNN.add(Dropout(0.2))\n",
    "modelCNN.add(Dense(128, activation=\"relu\"))\n",
    "modelCNN.add(Dropout(0.2))\n",
    "modelCNN.add(Dense(18, activation=\"softmax\"))\n",
    "modelCNN.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=Adam(lr=0.01),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(modelCNN.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = pad_sequences(tokenizer.texts_to_sequences(train_X),\n",
    "                                maxlen=max_sequence_length, padding='post')\n",
    "output_sequences = np.array(train_Y)\n",
    "print('Shape of data tensor:', input_sequences.shape)\n",
    "print('Shape of output tensor:', output_sequences.shape)\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "print('Training model...')\n",
    "z = np.zeros((len(input_sequences), LATENT_DIM))\n",
    "seed(1)\n",
    "set_random_seed(2)\n",
    "modelCNN.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer=Adam(lr=0.01),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "modelCNN.fit(\n",
    "  input_sequences,\n",
    "  output_sequences,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "pred_Y = model.predict(input_sequences).round()\n",
    "\n",
    "f1 = f1_score(output_sequences, pred_Y, average='micro')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_label_cnn_v1.json', \"w\") as json_file:\n",
    "        json_file.write(modelCNN.to_json())\n",
    "modelCNN.save_weights('model_label_cnn_v1.h5')\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_X, dev_Y = getLabelsData(dev_data)\n",
    "input_sequences_dev = pad_sequences(tokenizer.texts_to_sequences(dev_X),\n",
    "                                maxlen=max_sequence_length, padding='post')\n",
    "output_sequences_dev = np.array(dev_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = modelCNN.predict(input_sequences_dev).round()\n",
    "\n",
    "f1 = f1_score(output_sequences_dev, pred_Y, average='micro')\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
