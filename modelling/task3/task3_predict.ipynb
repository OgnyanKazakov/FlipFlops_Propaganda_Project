{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PACKAGE IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import io\n",
    "import pickle\n",
    "import re\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Embedding, Input, LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import groupby\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(\"article([0-9]+).*\")\n",
    "\n",
    "def loadDataTask3(folder):\n",
    "    result = []\n",
    "    fileNames = glob.glob(folder + \"/*.txt\")\n",
    "    for fileName in fileNames:\n",
    "        article = fileName.split(\"/\")[-1].split(\".\")[0]\n",
    "        articleId = regex.match(article).group(1)\n",
    "        f = open(fileName, \"r\", encoding=\"utf8\")\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "        sentences = [x for x in data.split(\"\\n\") if x != \"\"]\n",
    "        split = [x for x in splitWithIndices(data, \"\\n\")]\n",
    "        result.append({\"id\": articleId, \"data\": data, \"sentences\": sentences, \"split\": split})\n",
    "        \n",
    "    return result\n",
    "\n",
    "def splitWithIndices(s, c=' '):\n",
    "    p = 0\n",
    "    for k, g in groupby(s, lambda x:x==c):\n",
    "        q = p + sum(1 for i in g)\n",
    "        if not k:\n",
    "            yield p, q # or p, q-1 if you are really sure you want that\n",
    "        p = q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_data = loadDataTask3(\"train-split/tasks-2-3/train-dev/\")\n",
    "#dev_data = loadDataTask3(\"tasks-2-3/train/\")\n",
    "#dev_data = loadDataTask3(\"dev-INPUT/tasks-2-3/dev/\")\n",
    "dev_data = loadDataTask3(\"test-INPUT/tasks-2-3/test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREP ROUTINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenization\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "max_sequence_length = 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2index = {\n",
    "    \"Appeal_to_Authority\": 0,\n",
    "    \"Appeal_to_fear-prejudice\": 1,\n",
    "    \"Bandwagon\": 2,\n",
    "    \"Black-and-White_Fallacy\": 3,\n",
    "    \"Causal_Oversimplification\": 4,\n",
    "    \"Doubt\": 5,\n",
    "    \"Exaggeration,Minimisation\": 6,\n",
    "    \"Flag-Waving\": 7,\n",
    "    \"Loaded_Language\": 8,\n",
    "    \"Name_Calling,Labeling\": 9,\n",
    "    \"Obfuscation,Intentional_Vagueness,Confusion\": 10,\n",
    "    \"Red_Herring\": 11,\n",
    "    \"Reductio_ad_hitlerum\": 12,\n",
    "    \"Repetition\": 13,\n",
    "    \"Slogans\": 14,\n",
    "    \"Straw_Men\": 15,\n",
    "    \"Thought-terminating_Cliches\": 16,\n",
    "    \"Whataboutism\": 17\n",
    "}\n",
    "\n",
    "index2label = [\n",
    "    \"Appeal_to_Authority\",\n",
    "    \"Appeal_to_fear-prejudice\",\n",
    "    \"Bandwagon\",\n",
    "    \"Black-and-White_Fallacy\",\n",
    "    \"Causal_Oversimplification\",\n",
    "    \"Doubt\",\n",
    "    \"Exaggeration,Minimisation\",\n",
    "    \"Flag-Waving\",\n",
    "    \"Loaded_Language\",\n",
    "    \"Name_Calling,Labeling\",\n",
    "    \"Obfuscation,Intentional_Vagueness,Confusion\",\n",
    "    \"Red_Herring\",\n",
    "    \"Reductio_ad_hitlerum\",\n",
    "    \"Repetition\",\n",
    "    \"Slogans\",\n",
    "    \"Straw_Men\",\n",
    "    \"Thought-terminating_Cliches\",\n",
    "    \"Whataboutism\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 25\n",
    "\n",
    "def updateSentence(sentence, char, start=None, stop=None):\n",
    "    if start is None:\n",
    "        start = 0;\n",
    "        \n",
    "    if stop is None:\n",
    "        stop = len(sentence)\n",
    "        \n",
    "    s = list(sentence)\n",
    "    for i in range(start, stop):\n",
    "        if i < len(s):\n",
    "            if s[i] in '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ':\n",
    "                s[i] = \" \"\n",
    "            if s[i] not in [\" \", \"\\n\"]:\n",
    "                s[i] = char\n",
    "    return \"\".join(s)\n",
    "\n",
    "def word2label(word):\n",
    "    if word[0] == \"A\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def sen2label(sen):\n",
    "    result = [word2label(x) for x in sen if x != \"\"]\n",
    "    return result\n",
    "\n",
    "def predictModel(tokenizer, data, version, result):\n",
    "    pred_X = []\n",
    "    pred_loc = []\n",
    "    for x in data:\n",
    "        pred_X += x[\"sentences\"]\n",
    "        pred_loc += [(x[\"id\"], y[0], y[1]) for y in x[\"split\"]]\n",
    "    \n",
    "    input_sequences = pad_sequences(tokenizer.texts_to_sequences(pred_X),\n",
    "                                    maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    print('Shape of data tensor:', input_sequences.shape)\n",
    "    \n",
    "    # load json and create model\n",
    "    json_file = open('model_prob_cnn_{:}.json'.format(version), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights('model_prob_cnn_{:}.h5'.format(version))\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "\n",
    "    pred_Y = model.predict(input_sequences).round().astype(int)\n",
    "    pred_Y = np.reshape(pred_Y, pred_Y.shape[:2])    \n",
    "        \n",
    "    for i, x in enumerate(pred_Y):\n",
    "        if sum(x) > 0:\n",
    "            articleId = pred_loc[i][0]\n",
    "            sen_start = pred_loc[i][1]\n",
    "            sen = pred_X[i]\n",
    "            pos = extractIndices(sen, x, sen_start)\n",
    "            for p in pos:\n",
    "                result.append([articleId, str(p[0]), str(p[1]), sen[(p[0] - sen_start):(p[1] - sen_start)]])\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractIndices(sentence, pred, sen_start):\n",
    "    s = list(sentence)\n",
    "    for i in range(len(s)):\n",
    "        if s[i] in '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ':\n",
    "            s[i] = \" \"\n",
    "    #word split\n",
    "    split = [x for x in splitWithIndices(s)]\n",
    "    \n",
    "    #pred indices\n",
    "    idx = [i for i in range(len(split)) if pred[i] == 1]\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    prev = None\n",
    "    start = []\n",
    "    end = []\n",
    "    for x in idx:\n",
    "        if prev is None:\n",
    "            start.append(x)\n",
    "        elif prev + 1 < x:\n",
    "            end.append(prev)\n",
    "            start.append(x)\n",
    "        prev = x\n",
    "    end.append(idx[-1])\n",
    "    \n",
    "    labels = [x for x in zip(start, end) if x[0] < x[1]]\n",
    "    return [(split[l[0]][0] + sen_start, split[l[1]][1] + sen_start) for l in labels]\n",
    "        \n",
    "    \n",
    "def predictLabel(data, version):\n",
    "    with open('tokenizer_label.pickle', 'rb') as handle:\n",
    "        tokenizer_label = pickle.load(handle)\n",
    "    pred_X = [x[3] for x in data]\n",
    "    max_sequence_length = 81\n",
    "    input_sequences = pad_sequences(tokenizer_label.texts_to_sequences(pred_X),\n",
    "                                    maxlen=max_sequence_length, padding='post')\n",
    "    print('Shape of data tensor:', input_sequences.shape)\n",
    "    \n",
    "    # load json and create model\n",
    "    json_file = open('model_label_cnn_{:}.json'.format(version), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights('model_label_cnn_{:}.h5'.format(version))\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    \n",
    "    pred_Y = model.predict(input_sequences)\n",
    "    pred_Y = [np.argmax(x) for x in pred_Y]\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(pred_Y)):\n",
    "        result.append([data[i][0], index2label[pred_Y[i]], data[i][1], data[i][2]])\n",
    "    return result\n",
    "\n",
    "def exportResult(result):\n",
    "    with open(\"example-submission-task3-predictions_comb.txt\", \"w\") as fout:\n",
    "        for key, group in groupby(result, lambda x: x[0]):\n",
    "            for thing in group:\n",
    "                fout.write('\\t'.join(thing[0:4]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "result = predictModel(tokenizer, dev_data, \"v1\", result)\n",
    "result = predictLabel(result, \"v1\")\n",
    "# Remove types for which we do not perform well\n",
    "result = [x for x in result if x[1] not in [\"Appeal_to_fear-prejudice\", \n",
    "                                            \"Appeal_to_Authority\", \n",
    "                                            \"Obfuscation,Intentional_Vagueness,Confusion\",\n",
    "                                            \"Bandwagon\",\n",
    "                                            \"Reductio_ad_hitlerum\", \n",
    "                                            \"Straw_Men\", \n",
    "                                            \"Whataboutism\", \n",
    "                                            \"Red_Herring\", \n",
    "                                            \"Repetition\", \n",
    "                                            \"Thought-terminating_Cliches\", \n",
    "                                            \"Slogans\", \n",
    "                                            \"Causal_Oversimplification\"]]\n",
    "exportResult(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
