{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfblwROam5y4",
    "outputId": "28cb4b64-a080-48da-83b3-516418907c33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, GlobalMaxPool1D, Dense, Dropout\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, PredefinedSplit, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhE0uP7Xm5y9"
   },
   "outputs": [],
   "source": [
    "# Set Path\n",
    "path = os.path.abspath('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "flco5dWbm5zA"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprep_task2(path):\n",
    "    \"\"\"Dataprep for Task2 It will return the new data\n",
    "    :param path: Path to the article's taks3 labels file.\n",
    "    Example:\n",
    "    >>> dataprep_task2(\"datasets-v5/tasks-2-3/train/article111111112.task2.labels\")\n",
    "    \"\"\"\n",
    "    dir_name = os.path.dirname(path)\n",
    "    article_id = os.path.basename(path).split('.')[0]\n",
    "    article_name = os.path.join(dir_name, f'{article_id}.txt')\n",
    "\n",
    "    with open(article_name, 'r', encoding='utf8') as f:\n",
    "        records = f.readlines()\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['sentences'])\n",
    "\n",
    "    another_df = pd.read_csv(path, sep='\\t', names = ['article', 'N_sentence', 'is_propaganda'], encoding='utf8')\n",
    "    \n",
    "    result_df = pd.concat([df, another_df], axis=1)\n",
    "    \n",
    "    return result_df.loc[result_df['sentences'] != '\\n', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames = glob.glob(os.path.join(path, 'data', 'raw', 'tasks-2-3', 'train') + \"/*.task2.labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_list = list()\n",
    "\n",
    "for f in fileNames:\n",
    "    res_list.append(dataprep_task2(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pamela Geller and Robert Spencer co-founded anti-Muslim group Stop Islamization of America.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2]['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['sentences'] = df['sentences'].str.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4Hd81u9m5zH",
    "outputId": "e3d66be8-02c7-4371-c9d3-9d69c06af106"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14263, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>article</th>\n",
       "      <th>N_sentence</th>\n",
       "      <th>is_propaganda</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US bloggers banned from entering UK</td>\n",
       "      <td>111111112</td>\n",
       "      <td>1</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two prominent US bloggers have been banned fro...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>3</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pamela Geller and Robert Spencer co-founded an...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>5</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>They were due to speak at an English Defence L...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>7</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A government spokesman said individuals whose ...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>9</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences    article  N_sentence  \\\n",
       "0                US bloggers banned from entering UK  111111112           1   \n",
       "2  Two prominent US bloggers have been banned fro...  111111112           3   \n",
       "4  Pamela Geller and Robert Spencer co-founded an...  111111112           5   \n",
       "6  They were due to speak at an English Defence L...  111111112           7   \n",
       "8  A government spokesman said individuals whose ...  111111112           9   \n",
       "\n",
       "    is_propaganda  target  \n",
       "0  non-propaganda       0  \n",
       "2  non-propaganda       0  \n",
       "4      propaganda       1  \n",
       "6  non-propaganda       0  \n",
       "8  non-propaganda       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZxvU9c6OwyG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['target'] = df['is_propaganda'].map({'propaganda': 1, 'non-propaganda': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "tokenizer = cvec.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # Remove special chars and punctuation\n",
    "    text = \" \".join(tokenizer(text))\n",
    "    \n",
    "    # lowcase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Lematize\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    \n",
    "    # Lematize\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNWHPdH1OwyM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['sentences_prep'] = df['sentences'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['len'] = df['sentences_prep'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['len']>3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xf8gEvzH2p7E"
   },
   "source": [
    "## Make the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XeMsGUpV2p7F"
   },
   "outputs": [],
   "source": [
    "# The whole sample is split on 3 parts - dev, val, test\n",
    "art_id_dev, art_id_val = train_test_split(df['article'].unique(), test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "print(art_id_dev.size)\n",
    "print(art_id_val.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dev = df[df['article'].isin(art_id_dev)]\n",
    "df_val = df[df['article'].isin(art_id_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNs4PnWo2p7H"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_dev['sample'] = 'dev'\n",
    "df_val['sample'] = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9342, 8)\n",
      "(2804, 8)\n"
     ]
    }
   ],
   "source": [
    "# Sample sizes\n",
    "print(df_dev.shape)\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "carq5ueDOwyS",
    "outputId": "dcaba347-0bef-425f-dc4f-ed73a7b4790d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.400449582530507"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the length\n",
    "df_dev['sentences_prep'].apply(lambda x: len(x.split(\" \"))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9342.000000\n",
       "mean       13.400450\n",
       "std         8.032138\n",
       "min         4.000000\n",
       "25%         7.000000\n",
       "50%        12.000000\n",
       "75%        17.000000\n",
       "max        74.000000\n",
       "Name: sentences_prep, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['sentences_prep'].apply(lambda x: len(x.split(\" \"))).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vec and take the avg vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format(os.path.join('path', 'data', 'raw', 'GoogleNews-vectors-negative300.bin'), binary=True)\n",
    "vector_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_dev['sentences_vector'] = df_dev['sentences_prep'].apply(avg_vector)\n",
    "df_val['sentences_vector'] = df_val['sentences_prep'].apply(avg_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dev_vector_mean_list = list()\n",
    "for el in df_dev['sentences_vector']:\n",
    "    df_dev_vector_mean_list.append(el.tolist())\n",
    "    \n",
    "df_val_vector_mean_list = list()\n",
    "for el in df_val['sentences_vector']:\n",
    "    df_val_vector_mean_list.append(el.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dev_vector_mean_df = pd.DataFrame(df_dev_vector_mean_list, index=df_dev.index, columns=[\"vector_mean_\"+str(i) for i in range(300)])\n",
    "df_val_vector_mean_df = pd.DataFrame(df_val_vector_mean_list, index=df_val.index, columns=[\"vector_mean_\"+str(i) for i in range(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all together\n",
    "df_dev = pd.concat([df_dev, df_dev_vector_mean_df], axis=1)\n",
    "df_val = pd.concat([df_val, df_val_vector_mean_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wgtTsk42p7O"
   },
   "source": [
    "## Prepare for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8EXA6aF2p7S",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dev\n",
    "\n",
    "# Prepare the X\n",
    "df_dev_x = df_dev.loc[:, 'vector_mean_0':'vector_mean_299']\n",
    "\n",
    "# Prepare the y\n",
    "df_dev_y = df_dev['target'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_val[df_val['vector_mean_0'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKC9CyiH2p7V"
   },
   "outputs": [],
   "source": [
    "# Val\n",
    "\n",
    "# Prepare the X\n",
    "df_val_x = df_val.loc[:, 'vector_mean_0':'vector_mean_299']\n",
    "\n",
    "# Prepare the y\n",
    "df_val_y = df_val['target'].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params = {'subsample': 1, 'reg_alpha': 0.05, 'n_estimators': 100, 'max_depth': 8, 'learning_rate': 0.03, 'gamma': 10, 'colsample_bytree': 1}\n",
    "\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, max_delta_step=0, min_child_weight=20,\n",
    "                           missing=None, n_jobs=-1, nthread=-1, objective='binary:logistic', random_state=42, reg_lambda=1,\n",
    "                           scale_pos_weight=2.3210095982936365, seed=42, silent=True, subsample = 1, reg_alpha = 0.05, n_estimators = 100, \n",
    "                           max_depth = 8, learning_rate = 0.03, gamma = 10, colsample_bytree = 1\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=10, learning_rate=0.03, max_delta_step=0,\n",
       "       max_depth=8, min_child_weight=20, missing=None, n_estimators=100,\n",
       "       n_jobs=-1, nthread=-1, objective='binary:logistic', random_state=42,\n",
       "       reg_alpha=0.05, reg_lambda=1, scale_pos_weight=2.3210095982936365,\n",
       "       seed=42, silent=True, subsample=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cl.fit(df_dev_x, df_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "df_dev_y_pred = xgb_cl.predict(df_dev_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.8581818181818182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5731,   99],\n",
       "       [ 798, 2714]], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('F1-score: {0}'.format(f1_score(df_dev_y_pred, df_dev_y)))\n",
    "confusion_matrix(df_dev_y_pred, df_dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# val\n",
    "df_val_y_pred = xgb_cl.predict(df_val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.5352622061482821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1588,  297],\n",
       "       [ 474,  444]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('F1-score: {0}'.format(f1_score(df_val_y_pred, df_val_y)))\n",
    "confusion_matrix(df_val_y_pred, df_val_y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
