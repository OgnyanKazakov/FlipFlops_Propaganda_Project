{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /anaconda3/lib/python3.6/site-packages (0.15.2)\n",
      "Requirement already satisfied: nltk>=3.1 in /anaconda3/lib/python3.6/site-packages (from textblob) (3.2.4)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from nltk>=3.1->textblob) (1.11.0)\n",
      "Requirement already satisfied: textstat in /anaconda3/lib/python3.6/site-packages (0.5.4)\n",
      "Requirement already satisfied: pyphen in /anaconda3/lib/python3.6/site-packages (from textstat) (0.9.5)\n",
      "Requirement already satisfied: repoze.lru in /anaconda3/lib/python3.6/site-packages (from textstat) (0.7)\n",
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.6/site-packages (3.2.4)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied: spacy in /anaconda3/lib/python3.6/site-packages (2.0.18)\n",
      "Requirement already satisfied: regex==2018.01.10 in /anaconda3/lib/python3.6/site-packages (from spacy) (2018.1.10)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /anaconda3/lib/python3.6/site-packages (from spacy) (6.12.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.18.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.16.0)\n",
      "Requirement already satisfied: ujson>=1.35 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.29.1)\n",
      "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2017.7.27.1)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /anaconda3/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.8.2)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/spacy/__main__.py\", line 31, in <module>\n",
      "    plac.call(commands[command], sys.argv[1:])\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n",
      "    cmd, result = parser.consume(arglist)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n",
      "    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/spacy/cli/download.py\", line 36, in download\n",
      "    .format(m=model_name, v=version), pip_args)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/spacy/cli/download.py\", line 86, in download_model\n",
      "    return subprocess.call(cmd, env=os.environ.copy())\n",
      "  File \"/anaconda3/lib/python3.6/subprocess.py\", line 269, in call\n",
      "Requirement already satisfied: sklearn in /anaconda3/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /anaconda3/lib/python3.6/site-packages (from sklearn) (0.19.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!pip install textstat\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install sklearn\n",
    "\n",
    "!pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import os\n",
    "import sklearn\n",
    "import afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\C18244A\\Documents\\FlipFlops_Propaganda_Project\\datatask2_df_test.csv',\n",
    "                     usecols=['sentences', 'article', 'N_sentence', 'is_propaganda', \n",
    "                              'sentiment_polarity', 'sentiment_subjectivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentences', 'article', 'N_sentence', 'is_propaganda',\n",
       "       'sentiment_polarity', 'sentiment_subjectivity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4042, 6)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>article</th>\n",
       "      <th>N_sentence</th>\n",
       "      <th>is_propaganda</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Next plague outbreak in Madagascar could be 's...</td>\n",
       "      <td>111111111</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Geneva - The World Health Organisation chief o...</td>\n",
       "      <td>111111111</td>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"The next transmission could be more pronounce...</td>\n",
       "      <td>111111111</td>\n",
       "      <td>5</td>\n",
       "      <td>?</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An outbreak of both bubonic plague, which is s...</td>\n",
       "      <td>111111111</td>\n",
       "      <td>7</td>\n",
       "      <td>?</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Madagascar has suffered bubonic plague outbrea...</td>\n",
       "      <td>111111111</td>\n",
       "      <td>9</td>\n",
       "      <td>?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences    article  N_sentence  \\\n",
       "0  Next plague outbreak in Madagascar could be 's...  111111111           1   \n",
       "1  Geneva - The World Health Organisation chief o...  111111111           3   \n",
       "2  \"The next transmission could be more pronounce...  111111111           5   \n",
       "3  An outbreak of both bubonic plague, which is s...  111111111           7   \n",
       "4  Madagascar has suffered bubonic plague outbrea...  111111111           9   \n",
       "\n",
       "  is_propaganda  sentiment_polarity  sentiment_subjectivity  \n",
       "0             ?            0.000000                0.000000  \n",
       "1             ?           -0.066667                0.466667  \n",
       "2             ?            0.055556                0.388889  \n",
       "3             ?            0.150000                0.250000  \n",
       "4             ?            0.000000                0.000000  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_nlp(row):\n",
    "    nlp = en_core_web_sm.load()\n",
    "    temp = defaultdict(int)\n",
    "    temp.update(Counter(map(lambda x:x.label_, nlp(row).ents)))\n",
    "    return temp\n",
    "\n",
    "def iter_row(series):\n",
    "    return [apply_nlp(s) for s in series]\n",
    "\n",
    "def generate_features(df):\n",
    "    df['sentiment_polarity'] = df.apply(lambda x: TextBlob(x['sentences']).sentiment[0], axis=1)\n",
    "    df['sentiment_subjectivity'] = df.apply(lambda x: TextBlob(x['sentences']).sentiment[1], axis=1)\n",
    "\n",
    "    data = df.apply(lambda x: apply_nlp(x['sentences']), axis=1)\n",
    "    df_POS = pd.DataFrame(data.tolist())\n",
    "\n",
    "    df = df.merge(df_POS, left_index=True, right_index=True)\n",
    "    \n",
    "    df['flesch_reading_ease'] = df.apply(lambda x: textstat.flesch_reading_ease(x['sentences']), axis=1)\n",
    "    df['smog_index'] = df.apply(lambda x: textstat.smog_index(x['sentences']), axis=1)\n",
    "    df['flesch_kincaid_grade'] = df.apply(lambda x: textstat.flesch_kincaid_grade(x['sentences']), axis=1)\n",
    "    df['coleman_liau_index'] = df.apply(lambda x: textstat.coleman_liau_index(x['sentences']), axis=1)\n",
    "    df['automated_readability_index'] = df.apply(lambda x: textstat.automated_readability_index(x['sentences']), axis=1)\n",
    "    df['dale_chall_readability_score'] = df.apply(lambda x: textstat.dale_chall_readability_score(x['sentences']), axis=1)\n",
    "    df['difficult_words'] = df.apply(lambda x: textstat.difficult_words(x['sentences']), axis=1)\n",
    "    df['linsear_write_formula'] = df.apply(lambda x: textstat.linsear_write_formula(x['sentences']), axis=1)\n",
    "    df['gunning_fog'] = df.apply(lambda x: textstat.gunning_fog(x['sentences']), axis=1)\n",
    "    df['text_standard'] = df.apply(lambda x: textstat.text_standard(x['sentences']), axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = generate_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from itertools import chain\n",
    "import sys\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class DataTokenization:\n",
    "\n",
    "    def __init__(self, tickets):\n",
    "        \"\"\"\n",
    "        Init DataTokenization object\n",
    "\n",
    "        Args:\n",
    "            ds: Dataframe (pandas type) having only index and one column with single document per record.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(tickets, list):\n",
    "            raise Exception('The input should be of type list')\n",
    "\n",
    "        self.tickets = tickets\n",
    "        self.use_stemming = \"Yes\"\n",
    "        self.n_grams_type = 'simple'\n",
    "        self.n_grams = \"3\"\n",
    "        self.stopwords = stopwords.words('english')\n",
    "        self.use_phrases = \"Yes\"\n",
    "\n",
    "    def _document_tokenizer(self, document: str, m_stopwords: list, wordnet_lemmatizer,\n",
    "                            stemmer, synonyms_method):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            document: a document. Could be from single word to multiple lines/paragraphs text.\n",
    "            m_stopwords [Optional]: This is a list of the stopwords which should be excluded from the tokens\n",
    "                                  The default value is empty list which means no words will be excluded.\n",
    "            stemmer: [object] Stemmming object.         \n",
    "            wordnet_lemmatizer: [object] Lemmatizer object.         \n",
    "            synonyms_method [Optional]: How the synonyms are exctacted - using nltk or spacy\n",
    "\n",
    "        Returns:\n",
    "            A list of prepared tokens of the documnet.\n",
    "\n",
    "        Example:\n",
    "            input document:\n",
    "            'John.Doe@examplemail.com cannot log onto the SDE (or reset the password).\n",
    "            Emma raised ART request  for an SDE login for John.Doe@examplemail.com\n",
    "            This request has status \"COMPLETE\", but in the \"COMMENTS\" field there is a message \"Failed to add user Example.name to PORT_T_NEW\"\n",
    "\n",
    "            outputted tokens:\n",
    "            ['log', 'onto', 'sde', 'reset', 'password', 'rais', 'request', 'sde', 'login',\n",
    "            'request', 'status', 'complet', 'comment', 'messag', 'fail', 'add', 'user', 'examplename', 'porttnew']\n",
    "        \"\"\"     \n",
    "        \n",
    "        try:\n",
    "            # split string into words (tokens)\n",
    "            tokens = tokenize.word_tokenize(document)   \n",
    "\n",
    "            # remove any digits, i.e. \"3rd edition\"; remove any path, i.e. \"//math/lib\" and remove any path on PC\n",
    "            tokens = [t for t in tokens if not any((c.isdigit() or c == '/' or c == '\\\\') for c in t)] \n",
    "\n",
    "            # remove special chars\n",
    "            tokens = [re.sub(r'[^a-zA-Z0-9\\s]', '', t) for t in tokens]  \n",
    "            \n",
    "            # remove short words which don't contain capital letters. \n",
    "            tokens = [t for t in tokens \n",
    "                      if not (len(t) <= 2 and t != 'no' and t != 'nt' and (t.islower()) or t == '')] \n",
    "            \n",
    "            # downcase\n",
    "            tokens = [t.lower() for t in tokens]  \n",
    "            \n",
    "            # remove stopwords\n",
    "            tokens = [t for t in tokens if t not in m_stopwords]  \n",
    "            \n",
    "            # put words into base form\n",
    "            tokens = [wordnet_lemmatizer.lemmatize(t, 'v') for t in tokens]  \n",
    "\n",
    "            # put words into core form\n",
    "            # execute only if no synonyms are needed\n",
    "            # if both stemming and synonyms should be added - this is added in visual script\n",
    "            stremming = self.use_stemming.lower().strip()\n",
    "            phrases = elf.use_phrases.lower().strip()\n",
    "            if ((stremming == 'yes' or stremming == 'y') and (phrases != 'yes' or phrases!= 'y')):\n",
    "\n",
    "                tokens = [stemmer.stem(t) for t in tokens]  \n",
    "\n",
    "        except Exception as err:\n",
    "            raise Exception(\"There was an issue with tokenizing a document:\" + str(err))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _phrase_generator(self, wordnet_lemmatizer, stemmer):\n",
    "        \"\"\"\n",
    "        This method generates the phrases based on prepared tickets from \n",
    "        _document_tokenizer method.\n",
    "\n",
    "        Args:\n",
    "            wordnet_lemmatizer: [object] Lemmatizer object. \n",
    "\n",
    "         Returns:\n",
    "            A list of prepared phreses of the corpus.        \n",
    "        \"\"\"\n",
    "\n",
    "        corpus_all_phrases = \\\n",
    "            [self._document_tokenizer(self.tickets[doc], m_stopwords=self.stopwords,\n",
    "             wordnet_lemmatizer=wordnet_lemmatizer, stemmer='', synonyms_method='none') \n",
    "             for doc in range(len(self.tickets))]\n",
    "\n",
    "        docs = [' '.join([t for t in doc]) for doc in corpus_all_phrases]\n",
    "\n",
    "        # set spaCy object with wordnet annotator\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "        bad_deps = ('prep', 'aux')\n",
    "        phrase_list = []\n",
    "        phrase_list_per_doc = []\n",
    "\n",
    "        for doc in docs:\n",
    "            # take the phases using spaCy object with noun_chunks attribute\n",
    "            phrase_list_per_doc = []\n",
    "            for phrase in nlp(doc).noun_chunks:\n",
    "               \n",
    "                # exclude bad_deps\n",
    "                while len(phrase) > 1 and phrase[0].dep_ not in bad_deps:\n",
    "                    phrase = phrase[1:]\n",
    "\n",
    "                # check if our phrase have one or more word and            \n",
    "                if len(phrase) > 1 and len(str(phrase))>1:\n",
    "                    # Merge the tokens, e.g. good_ideas\n",
    "                    phrase = str(phrase.text).lower().replace(\" \", \"_\")\n",
    "                    phrase_list_per_doc.append(phrase)\n",
    "        \n",
    "                if  len(phrase_list_per_doc) > 0:    \n",
    "                    phrase_list.append(phrase_list_per_doc)\n",
    "                else:\n",
    "                     phrase_list.append(\"None\")\n",
    "        return phrase_list\n",
    "\n",
    "    def tickets_tokenizer(self):\n",
    "        # load object from the related packages - Lemmatizer\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # load object from the related packages - Stemming\n",
    "        if (self.use_stemming.lower().strip() == 'yes' or self.use_stemming.lower().strip() == 'y'):\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "        else:\n",
    "            stemmer = ''\n",
    "\n",
    "        # Perform the tokenization\n",
    "        corpus_all_tokens = \\\n",
    "            [self._document_tokenizer(self.tickets[doc], m_stopwords=self.stopwords,\n",
    "             wordnet_lemmatizer=wordnet_lemmatizer, stemmer=stemmer, synonyms_method='') \n",
    "             for doc in range(len(self.tickets))]\n",
    "\n",
    "        corpus_all_tokens = [' '.join(doc) for doc in corpus_all_tokens]\n",
    "        corpus_all_tokens = [doc if str(doc).strip() != '' else 'test' for doc in corpus_all_tokens]\n",
    "\n",
    "        # Create a list of list per token for bi-grams\n",
    "        n_grams_2, n_grams_3 = [], []\n",
    "\n",
    "        if (int(self.n_grams) != 0 and int(self.n_grams) != 2 and int(self.n_grams) != 3):\n",
    "            raise Exception((\"You need to enter valid value for n-grams: 0, 2 or 3 as \"\n",
    "                            \"integer value.s \\n You have used {self.n_grams} !\"))        \n",
    "\n",
    "        if (int(self.n_grams) == 2 or int(self.n_grams) == 3):\n",
    "\n",
    "            if (self.n_grams_type.lower().strip() == 'simple'):\n",
    "                n_grams_2 = [[a + \"_\" + b for a,b in ngrams(tokens.split(), 2)] for tokens in corpus_all_tokens]\n",
    "\n",
    "                if (int(self.n_grams) == 3): \n",
    "                    n_grams_3 = [[a + \"_\" + b + \"_\" + c for a,b,c in ngrams(tokens.split(), 3)] for tokens in corpus_all_tokens]\n",
    "            \n",
    "        n_grams_2 = [' '.join(doc) for doc in n_grams_2]      \n",
    "        n_grams_3 = [' '.join(doc) for doc in n_grams_3]\n",
    "\n",
    "        # Phrases \n",
    "        if (self.use_stemming.lower().strip() == 'yes'\n",
    "            or self.use_stemming.lower().strip() == 'y'):\n",
    "\n",
    "            try:\n",
    "                phrase_list = list(self._phrase_generator(wordnet_lemmatizer = wordnet_lemmatizer, stemmer = stemmer))\n",
    "                phrase_list = [' '.join(set_phrases) if set_phrases != \"None\" else set_phrases for set_phrases in phrase_list]\n",
    "            except Exception as err:\n",
    "                raise Exception(\"There was an issue with separatig the key phreses:\" + str(err))\n",
    "\n",
    "        return (corpus_all_tokens, n_grams_2, n_grams_3, phrase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the tickets data\n",
    "data = df\n",
    "\n",
    "# take the text unstructured part of the dataset\n",
    "# this df format with the index from the original dataset\n",
    "documents = data['sentences'].tolist()\n",
    "\n",
    "# load the DataPreprocessing object - the main DataParser object (by now)\n",
    "data_parser_object = DataTokenization(documents)\n",
    "\n",
    "corpus_all_tokens, n_grams_2, n_grams_3, phrase_list = data_parser_object.tickets_tokenizer()\n",
    "\n",
    "corpus_all_tokens_df = pd.DataFrame(corpus_all_tokens, columns=['corpus_all_tokens'])\n",
    "corpus_all_tokens_df.to_csv(\"corpus_all_tokens_df2.csv\")\n",
    "\n",
    "n_grams_2_df = pd.DataFrame(n_grams_2, columns=['n_grams_2'])\n",
    "n_grams_2_df.to_csv(\"n_grams_2_df2.csv\")\n",
    "\n",
    "n_grams_3_df = pd.DataFrame(n_grams_3, columns=['n_grams_3'])\n",
    "n_grams_3_df.to_csv(\"n_grams_3_df2.csv\")\n",
    "\n",
    "phrase_list_df= pd.DataFrame(phrase_list, columns=['phrase_list'])\n",
    "phrase_list_df.to_csv(\"phrase_list2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Combine Semantic Features with Emotional Features.ipynb'\n",
      " corpus_all_tokens_df2.csv\n",
      "'Dataprep for task 2-3.ipynb'\n",
      " dataprepT23.py\n",
      "'Download the data.ipynb'\n",
      "'Feature Creations.ipynb'\n",
      " final_data.pkl\n",
      "\"Fix Ogi's problem.ipynb\"\n",
      " Merge_Preped_Data_Case2.ipynb\n",
      " n_grams_2_df2.csv\n",
      " n_grams_3_df2.csv\n",
      " phrase_list2.csv\n",
      " __pycache__\n",
      "'R Demo.ipynb'\n",
      " Untitled.ipynb\n",
      "'Update dataprep for Dobi.ipynb'\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_dataset = pd.concat([corpus_all_tokens_df, n_grams_2_df, n_grams_3_df], axis=1, join = 'inner')\n",
    "combined_dataset.to_csv(\"corpus_all_tokens_n_grams_df2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate afinn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "af = afinn.Afinn()\n",
    "\n",
    "df['affin_on_article'] = df.apply(lambda x: af.score(text=x['article']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentences', 'article', 'N_sentence', 'is_propaganda', 'affin_score',\n",
       "       'sentiment_polarity', 'sentiment_subjectivity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combined_dataset = pd.read_csv('../generated_data/Ogi\\'s folder/new data/corpus_all_tokens_n_grams_df2.csv', \n",
    "#                                             usecols=['corpus_all_tokens', 'n_grams_2', 'n_grams_3'])\n",
    "\n",
    "corpus_all_tokens_n_grams_df2 = combined_dataset\n",
    "corpus_all_tokens_n_grams_df2.shape\n",
    "corpus_all_tokens_n_grams_df2.head()\n",
    "\n",
    "### Load semantic variables\n",
    "\n",
    "#semantic_vars_df = pd.read_csv(\"../generated_data/Ogi\\'s folder/new data/df_task2_with_features.csv\")\n",
    "semantic_vars_df = df\n",
    "semantic_vars_df.shape\n",
    "semantic_vars_df.head(5)\n",
    "semantic_vars_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop_columns = ['Unnamed: 0', 'index', 'Unnamed: 0.1', 'Unnamed: 0.2', '0']\n",
    "\n",
    "#semantic_vars_df = semantic_vars_df.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create total_df by joinning Corpus_Tokens and Phrase_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14263, 10)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df = semantic_vars_df.join(corpus_all_tokens_n_grams_df2)\n",
    "\n",
    "total_df.head(3)\n",
    "\n",
    "total_df.columns\n",
    "\n",
    "total_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new feature - sentece_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentences', 'article', 'N_sentence', 'is_propaganda', 'affin_score',\n",
       "       'sentiment_polarity', 'sentiment_subjectivity', 'corpus_all_tokens',\n",
       "       'n_grams_2', 'n_grams_3', 'corpus_len'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df['corpus_len'] = total_df.apply(lambda x: len(x['corpus_all_tokens'].strip()), axis=1)\n",
    "\n",
    "total_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    us bloggers ban enter uk us_bloggers bloggers_...\n",
       "1    two prominent us bloggers ban enter uk home of...\n",
       "2    pamela geller robert spencer cofounded antimus...\n",
       "3    due speak english defence league march woolwic...\n",
       "Name: all_tokens, dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def full_text(row):\n",
    "    return ' '.join(map(str, [row['corpus_all_tokens'], row['n_grams_2'], row['n_grams_3']]))\n",
    "\n",
    "total_df['all_tokens'] = total_df.apply(lambda x: full_text(x), axis=1)\n",
    "\n",
    "total_df['all_tokens'][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of uniqure words in the corpus: 232860\n"
     ]
    }
   ],
   "source": [
    "corpus_all_tokens_unique = set([w for doc in total_df['all_tokens'] for w in doc.split()])\n",
    "\n",
    "print(\"Total number of uniqure words in the corpus: \" + str(len(corpus_all_tokens_unique)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word2index and tf_idf dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14263x232860 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 424441 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the words in the dictionary in order to create keys DB table \n",
    "word2idx = {word:index for index, word in enumerate(corpus_all_tokens_unique)}\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_object = TfidfVectorizer(decode_error='ignore', vocabulary=word2idx)\n",
    "\n",
    "X_tfidf = tfidf_object.fit_transform(total_df['all_tokens'])\n",
    "\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>232850</th>\n",
       "      <th>232851</th>\n",
       "      <th>232852</th>\n",
       "      <th>232853</th>\n",
       "      <th>232854</th>\n",
       "      <th>232855</th>\n",
       "      <th>232856</th>\n",
       "      <th>232857</th>\n",
       "      <th>232858</th>\n",
       "      <th>232859</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 232860 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0       1       2       3       4       5       6       7       8       \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   9        ...    232850  232851  232852  232853  232854  232855  232856  \\\n",
       "0     0.0   ...       0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0   ...       0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0   ...       0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0   ...       0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   232857  232858  232859  \n",
       "0     0.0     0.0     0.0  \n",
       "1     0.0     0.0     0.0  \n",
       "2     0.0     0.0     0.0  \n",
       "3     0.0     0.0     0.0  \n",
       "\n",
       "[4 rows x 232860 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "\n",
    "X_tfidf_df.shape\n",
    "\n",
    "X_tfidf_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD Afinn Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentences', 'article', 'N_sentence', 'is_propaganda', 'affin_score',\n",
       "       'sentiment_polarity', 'sentiment_subjectivity', 'corpus_all_tokens',\n",
       "       'n_grams_2', 'n_grams_3', 'corpus_len', 'all_tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_afinn = pd.read_csv('../generated_data/data_case2_with_affin.csv')\n",
    "\n",
    "data_with_afinn.head(4)\n",
    "\n",
    "total_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14263, 12)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_afinn = data_with_afinn[['article', 'N_sentence',  'is_propaganda', 'affin_score']]\n",
    "\n",
    "data_with_afinn.head(5)\n",
    "\n",
    "total_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14263, 4)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_afinn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentences', 'article', 'N_sentence', 'is_propaganda', 'affin_score_x',\n",
       "       'sentiment_polarity', 'sentiment_subjectivity', 'corpus_all_tokens',\n",
       "       'n_grams_2', 'n_grams_3', 'corpus_len', 'all_tokens', 'affin_score_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = total_df.merge(data_with_afinn, how='inner', on=['article', 'N_sentence',  'is_propaganda'])\n",
    "\n",
    "new_data.shape\n",
    "\n",
    "new_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Linear optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-c940e5c09684>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tfidf_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# list of list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/decomposition/truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    175\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[1;32m    176\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                                           random_state=random_state)\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown algorithm %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     Q = randomized_range_finder(M, n_random, n_iter,\n\u001b[0;32m--> 365\u001b[0;31m                                 power_iteration_normalizer, random_state)\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;31m# Sample the range of A using by linear projection of Q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;31m# Extract an orthonormal basis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'economic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=600, n_iter=2, random_state = 1234)\n",
    "\n",
    "X_matrix = lsa.fit_transform(X_tfidf_df) # list of list \n",
    "\n",
    "type(X_matrix)\n",
    "\n",
    "X_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_data = new_data.merge(pd.DataFrame(X_matrix), left_index=True, right_index=True)\n",
    "\n",
    "total_data.shape\n",
    "\n",
    "total_data.columns\n",
    "\n",
    "columns = total_data.columns\n",
    "\n",
    "print (columns[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total_data = total_data.drop([ 'corpus_all_tokens', 'n_grams_2', 'n_grams_3', 'all_tokens', 'text_standard'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_data = total_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final data for Ogi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_data.to_csv('../final_data.csv', index=False)\n",
    "\n",
    "with open('../final_data.pkl', 'wb') as f:\n",
    "    pickle.dump(total_data, f)\n",
    "\n",
    "    '''\n",
    "X = total_data.iloc[:, 4:].values\n",
    "\n",
    "y = total_data['is_propaganda'].map({'non-propaganda':0, 'propaganda':1})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Fitting Kernel SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_s = f1_score(y_test, y_pred)\n",
    "print(\"Kernal SVM F1 result: \" + str(f1_s))\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('../model.pkl', 'wb') as f:\n",
    "    pickle.dump(classifier, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
