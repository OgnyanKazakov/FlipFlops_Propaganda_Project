{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/c1/cb652aacd55290eecd2a3e28f311dac5ce61f243ecd535a133db53499247/s3fs-0.2.0.tar.gz (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 2.4MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting botocore (from s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/66/38c903912870df4cdead76841041c1107252be129c75dbb533ce469572ec/botocore-1.12.85-py2.py3-none-any.whl (5.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.2MB 11.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting boto3 (from s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/0c/ab877fa34bebf8aba5af6e822c081972d11194be026a247997de3764ade4/boto3-1.9.85-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 52.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from s3fs) (1.12.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20; python_version >= \"3.4\" in /opt/conda/lib/python3.6/site-packages (from botocore->s3fs) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore->s3fs) (2.7.5)\n",
      "Collecting docutils>=0.10 (from botocore->s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
      "\u001b[K    100% |████████████████████████████████| 552kB 43.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from botocore->s3fs)\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 27.8MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: s3fs\n",
      "  Running setup.py bdist_wheel for s3fs ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/f8/28/1e/8cd0a0ea152c17b3e75cfdc2e4015168e13ac315ecb02bb591\n",
      "Successfully built s3fs\n",
      "Installing collected packages: docutils, jmespath, botocore, s3transfer, boto3, s3fs\n",
      "Successfully installed boto3-1.9.85 botocore-1.12.85 docutils-0.14 jmespath-0.9.3 s3fs-0.2.0 s3transfer-0.1.13\n"
     ]
    }
   ],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(anon = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['propaganda-datathon/dataset/Init-jupyter-environment-and-Get-dataset.ipynb',\n",
       " 'propaganda-datathon/dataset/datasets-v5.zip',\n",
       " 'propaganda-datathon/dataset/dev-INPUT.zip',\n",
       " 'propaganda-datathon/dataset/tools-v2.zip']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.ls('propaganda-datathon/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_block() missing 2 required positional arguments: 'offset' and 'length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d0dc8d6bc2cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'propaganda-datathon/dataset/datasets-v5.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: read_block() missing 2 required positional arguments: 'offset' and 'length'"
     ]
    }
   ],
   "source": [
    "fs.read_block('propaganda-datathon/dataset/datasets-v5.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('DataTask2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'sentences', 'article', 'N_sentence', 'is_propaganda'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentences</th>\n",
       "      <th>article</th>\n",
       "      <th>N_sentence</th>\n",
       "      <th>is_propaganda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>US bloggers banned from entering UK\\n</td>\n",
       "      <td>111111112</td>\n",
       "      <td>1</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Two prominent US bloggers have been banned fro...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>3</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Pamela Geller and Robert Spencer co-founded an...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>5</td>\n",
       "      <td>propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>They were due to speak at an English Defence L...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>7</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>A government spokesman said individuals whose ...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>9</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          sentences    article  \\\n",
       "0           0              US bloggers banned from entering UK\\n  111111112   \n",
       "1           2  Two prominent US bloggers have been banned fro...  111111112   \n",
       "2           4  Pamela Geller and Robert Spencer co-founded an...  111111112   \n",
       "3           6  They were due to speak at an English Defence L...  111111112   \n",
       "4           8  A government spokesman said individuals whose ...  111111112   \n",
       "\n",
       "   N_sentence   is_propaganda  \n",
       "0           1  non-propaganda  \n",
       "1           3  non-propaganda  \n",
       "2           5      propaganda  \n",
       "3           7  non-propaganda  \n",
       "4           9  non-propaganda  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df2.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['US bloggers banned from entering UK\\n',\n",
       " 'Two prominent US bloggers have been banned from entering the UK, the Home Office has said.\\n',\n",
       " 'Pamela Geller and Robert Spencer co-founded anti-Muslim group Stop Islamization of America.\\n',\n",
       " 'They were due to speak at an English Defence League march in Woolwich, where Drummer Lee Rigby was killed.\\n',\n",
       " 'A government spokesman said individuals whose presence \"is not conducive to the public good\" could be excluded by the home secretary.\\n',\n",
       " 'He added: \"We condemn all those whose behaviours and views run counter to our shared values and will not stand for extremism in any form.\"\\n',\n",
       " \"'Right decision'\\n\",\n",
       " 'Ms Geller, of the Atlas Shrugs blog, and Mr Spencer, of Jihad Watch, are also co-founders of the American Freedom Defense Initiative, best known for a pro-Israel \"Defeat Jihad\" poster campaign on the New York subway.\\n',\n",
       " 'On both of their blogs the pair called their bans from entering the UK \"a striking blow against freedom\" and said the \"the nation that gave the world the Magna Carta is dead\".\\n',\n",
       " 'They were due to attend a march planned by the far-right EDL to mark Armed Forces Day on 29 June, ending in Woolwich, south east London, where soldier Drummer Rigby was murdered last month.\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('corpus_all_tokens_df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>corpus_all_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>us bloggers ban enter uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>two prominent us bloggers ban enter uk home of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pamela geller robert spencer cofounded antimus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>due speak english defence league march woolwic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>government spokesman say individuals whose pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>add condemn whose behaviours view run counter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>right decision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>ms geller atlas shrug blog mr spencer jihad wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>blog pair call ban enter uk strike blow freedo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>due attend march plan farright edl mark arm fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                  corpus_all_tokens\n",
       "0           0                           us bloggers ban enter uk\n",
       "1           1  two prominent us bloggers ban enter uk home of...\n",
       "2           2  pamela geller robert spencer cofounded antimus...\n",
       "3           3  due speak english defence league march woolwic...\n",
       "4           4  government spokesman say individuals whose pre...\n",
       "5           5  add condemn whose behaviours view run counter ...\n",
       "6           6                                     right decision\n",
       "7           7  ms geller atlas shrug blog mr spencer jihad wa...\n",
       "8           8  blog pair call ban enter uk strike blow freedo...\n",
       "9           9  due attend march plan farright edl mark arm fo..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_2 = pd.read_csv('n_grams_2_df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_all_tokens_n_grams_df2 = pd.read_csv('corpus_all_tokens_n_grams_df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>corpus_all_tokens</th>\n",
       "      <th>n_grams_2</th>\n",
       "      <th>n_grams_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>us bloggers ban enter uk</td>\n",
       "      <td>us_bloggers bloggers_ban ban_enter enter_uk</td>\n",
       "      <td>us_bloggers_ban bloggers_ban_enter ban_enter_uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>two prominent us bloggers ban enter uk home of...</td>\n",
       "      <td>two_prominent prominent_us us_bloggers blogger...</td>\n",
       "      <td>two_prominent_us prominent_us_bloggers us_blog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pamela geller robert spencer cofounded antimus...</td>\n",
       "      <td>pamela_geller geller_robert robert_spencer spe...</td>\n",
       "      <td>pamela_geller_robert geller_robert_spencer rob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>due speak english defence league march woolwic...</td>\n",
       "      <td>due_speak speak_english english_defence defenc...</td>\n",
       "      <td>due_speak_english speak_english_defence englis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>government spokesman say individuals whose pre...</td>\n",
       "      <td>government_spokesman spokesman_say say_individ...</td>\n",
       "      <td>government_spokesman_say spokesman_say_individ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                  corpus_all_tokens  \\\n",
       "0           0                           us bloggers ban enter uk   \n",
       "1           1  two prominent us bloggers ban enter uk home of...   \n",
       "2           2  pamela geller robert spencer cofounded antimus...   \n",
       "3           3  due speak english defence league march woolwic...   \n",
       "4           4  government spokesman say individuals whose pre...   \n",
       "\n",
       "                                           n_grams_2  \\\n",
       "0        us_bloggers bloggers_ban ban_enter enter_uk   \n",
       "1  two_prominent prominent_us us_bloggers blogger...   \n",
       "2  pamela_geller geller_robert robert_spencer spe...   \n",
       "3  due_speak speak_english english_defence defenc...   \n",
       "4  government_spokesman spokesman_say say_individ...   \n",
       "\n",
       "                                           n_grams_3  \n",
       "0    us_bloggers_ban bloggers_ban_enter ban_enter_uk  \n",
       "1  two_prominent_us prominent_us_bloggers us_blog...  \n",
       "2  pamela_geller_robert geller_robert_spencer rob...  \n",
       "3  due_speak_english speak_english_defence englis...  \n",
       "4  government_spokesman_say spokesman_say_individ...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_all_tokens_n_grams_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list2_df = pd.read_csv(\"phrase_list2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>phrase_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 phrase_list\n",
       "0           0        None\n",
       "1           1        None\n",
       "2           2        None\n",
       "3           3        None\n",
       "4           4        None"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_list2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = corpus_all_tokens_n_grams_df2.join(phrase_list2_df.phrase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14291, 4)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_all_tokens_n_grams_df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fulltext'], dtype='object')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = total_df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['corpus_all_tokens'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-3b357883b0a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus_all_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corpus_all_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2726\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['corpus_all_tokens'] not in index\""
     ]
    }
   ],
   "source": [
    "corpus_all_tokens = total_df[['corpus_all_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_all_tokens_n_grams_df2['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_sentence = [len(doc.split()) for doc in corpus_all_tokens_n_grams_df2['corpus_all_tokens'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df['sentence_length'] = len_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "total_df['fulltext'] = pd.Series([])\n",
    "for i in range(total_df.shape[0]):\n",
    "    if total_df.phrase_list[i].strip() != \"None\":\n",
    "        total_df['fulltext'][i] = str(total_df.corpus_all_tokens[i]) + \" \" + str(total_df.n_grams_2[i]) + \" \" + str(total_df.n_grams_3[i]) + \" \" + str(total_df.phrase_list[i])\n",
    "    else:\n",
    "        total_df['fulltext'][i] = str(total_df.corpus_all_tokens[i]) + \" \" + str(total_df.n_grams_2[i]) + \" \" + str(total_df.n_grams_3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corpus_all_tokens    add condemn whose behaviours view run counter ...\n",
       "n_grams_2            add_condemn condemn_whose whose_behaviours beh...\n",
       "n_grams_3            add_condemn_whose condemn_whose_behaviours who...\n",
       "phrase_list                                                       None\n",
       "fulltext             add condemn whose behaviours view run counter ...\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df.iloc[5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = list(total_df['fulltext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14291"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_sentence = [len(doc.split()) for doc in all_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 27, 27, 30, 36, 33, 4, 72, 45, 63]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_sentence[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['us bloggers ban enter uk us_bloggers bloggers_ban ban_enter enter_uk us_bloggers_ban bloggers_ban_enter ban_enter_uk',\n",
       " 'two prominent us bloggers ban enter uk home office say two_prominent prominent_us us_bloggers bloggers_ban ban_enter enter_uk uk_home home_office office_say two_prominent_us prominent_us_bloggers us_bloggers_ban bloggers_ban_enter ban_enter_uk enter_uk_home uk_home_office home_office_say',\n",
       " 'pamela geller robert spencer cofounded antimuslim group stop islamization america pamela_geller geller_robert robert_spencer spencer_cofounded cofounded_antimuslim antimuslim_group group_stop stop_islamization islamization_america pamela_geller_robert geller_robert_spencer robert_spencer_cofounded spencer_cofounded_antimuslim cofounded_antimuslim_group antimuslim_group_stop group_stop_islamization stop_islamization_america',\n",
       " 'due speak english defence league march woolwich drummer lee rigby kill due_speak speak_english english_defence defence_league league_march march_woolwich woolwich_drummer drummer_lee lee_rigby rigby_kill due_speak_english speak_english_defence english_defence_league defence_league_march league_march_woolwich march_woolwich_drummer woolwich_drummer_lee drummer_lee_rigby lee_rigby_kill',\n",
       " 'government spokesman say individuals whose presence conducive public good could exclude home secretary government_spokesman spokesman_say say_individuals individuals_whose whose_presence presence_conducive conducive_public public_good good_could could_exclude exclude_home home_secretary government_spokesman_say spokesman_say_individuals say_individuals_whose individuals_whose_presence whose_presence_conducive presence_conducive_public conducive_public_good public_good_could good_could_exclude could_exclude_home exclude_home_secretary',\n",
       " 'add condemn whose behaviours view run counter share value stand extremism form add_condemn condemn_whose whose_behaviours behaviours_view view_run run_counter counter_share share_value value_stand stand_extremism extremism_form add_condemn_whose condemn_whose_behaviours whose_behaviours_view behaviours_view_run view_run_counter run_counter_share counter_share_value share_value_stand value_stand_extremism stand_extremism_form',\n",
       " 'right decision right_decision nan',\n",
       " 'ms geller atlas shrug blog mr spencer jihad watch also cofounders american freedom defense initiative best know proisrael defeat jihad poster campaign new york subway ms_geller geller_atlas atlas_shrug shrug_blog blog_mr mr_spencer spencer_jihad jihad_watch watch_also also_cofounders cofounders_american american_freedom freedom_defense defense_initiative initiative_best best_know know_proisrael proisrael_defeat defeat_jihad jihad_poster poster_campaign campaign_new new_york york_subway ms_geller_atlas geller_atlas_shrug atlas_shrug_blog shrug_blog_mr blog_mr_spencer mr_spencer_jihad spencer_jihad_watch jihad_watch_also watch_also_cofounders also_cofounders_american cofounders_american_freedom american_freedom_defense freedom_defense_initiative defense_initiative_best initiative_best_know best_know_proisrael know_proisrael_defeat proisrael_defeat_jihad defeat_jihad_poster jihad_poster_campaign poster_campaign_new campaign_new_york new_york_subway',\n",
       " 'blog pair call ban enter uk strike blow freedom say nation give world magna carta dead blog_pair pair_call call_ban ban_enter enter_uk uk_strike strike_blow blow_freedom freedom_say say_nation nation_give give_world world_magna magna_carta carta_dead blog_pair_call pair_call_ban call_ban_enter ban_enter_uk enter_uk_strike uk_strike_blow strike_blow_freedom blow_freedom_say freedom_say_nation say_nation_give nation_give_world give_world_magna world_magna_carta magna_carta_dead',\n",
       " 'due attend march plan farright edl mark arm force day june end woolwich south east london soldier drummer rigby murder last month due_attend attend_march march_plan plan_farright farright_edl edl_mark mark_arm arm_force force_day day_june june_end end_woolwich woolwich_south south_east east_london london_soldier soldier_drummer drummer_rigby rigby_murder murder_last last_month due_attend_march attend_march_plan march_plan_farright plan_farright_edl farright_edl_mark edl_mark_arm mark_arm_force arm_force_day force_day_june day_june_end june_end_woolwich end_woolwich_south woolwich_south_east south_east_london east_london_soldier london_soldier_drummer soldier_drummer_rigby drummer_rigby_murder rigby_murder_last murder_last_month']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of uniqure words in the corpus: 232889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus_all_tokens_unique = set([w for doc in all_tokens for w in doc.split()])\n",
    "print(\"Total number of uniqure words in the corpus: \" + str(len(corpus_all_tokens_unique)))\n",
    "\n",
    "# encode the words in the dictionary in order to create keys DB table \n",
    "word2idx = {w:i for i,w in enumerate(corpus_all_tokens_unique)}\n",
    "tfidf_object = TfidfVectorizer(decode_error='ignore', vocabulary = word2idx)\n",
    "X_tfidf = tfidf_object.fit_transform(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14291, 232889)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-fe0940de0a3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_tfidf.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1745\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    298\u001b[0m                                   \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                                   \u001b[0mdate_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                                   quoting=self.quoting)\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mto_native_types\u001b[0;34m(self, slicer, na_rep, float_format, decimal, quoting, **kwargs)\u001b[0m\n\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2067\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2068\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test.to_csv(\"X_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=600, n_iter=2, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix = lsa.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix_df = pd.DataFrame(X_matrix)\n",
    "X_matrix_df.to_csv('X_matrix_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14291, 600)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_matrix_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = pd.read_csv(\"df_task2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'sentence_id', 'sentences', 'article_id',\n",
       "       'Unnamed: 4', 'label', 'flesch_reading_ease', 'smog_index',\n",
       "       'flesch_kincaid_grade', 'coleman_liau_index',\n",
       "       'automated_readability_index', 'dale_chall_readability_score',\n",
       "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
       "       'text_standard', 'sentiment_polarity', 'sentiment_subjectivity',\n",
       "       'Unnamed: 0.2', '0', 'GPE', 'CARDINAL', 'ORG', 'NORP', 'PERSON', 'DATE',\n",
       "       'TIME', 'LOC', 'ORDINAL', 'EVENT', 'WORK_OF_ART', 'FAC', 'LAW',\n",
       "       'PRODUCT', 'MONEY', 'QUANTITY', 'PERCENT', 'LANGUAGE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'sentence_id', 'sentences', 'article_id',\n",
       "       'Unnamed: 4', 'label', 'flesch_reading_ease', 'smog_index',\n",
       "       'flesch_kincaid_grade', 'coleman_liau_index',\n",
       "       'automated_readability_index', 'dale_chall_readability_score',\n",
       "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
       "       'text_standard', 'sentiment_polarity', 'sentiment_subjectivity',\n",
       "       'Unnamed: 0.2', '0', 'GPE', 'CARDINAL', 'ORG', 'NORP', 'PERSON', 'DATE',\n",
       "       'TIME', 'LOC', 'ORDINAL', 'EVENT', 'WORK_OF_ART', 'FAC', 'LAW',\n",
       "       'PRODUCT', 'MONEY', 'QUANTITY', 'PERCENT', 'LANGUAGE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = total_dataset[['sentence_id', 'article_id',\n",
    "       'label', 'flesch_reading_ease', 'smog_index',\n",
    "       'flesch_kincaid_grade', 'coleman_liau_index',\n",
    "       'automated_readability_index', 'dale_chall_readability_score',\n",
    "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
    "       'text_standard', 'sentiment_polarity', 'sentiment_subjectivity',\n",
    "       'Unnamed: 0.2', '0', 'GPE', 'CARDINAL', 'ORG', 'NORP', 'PERSON', 'DATE',\n",
    "       'TIME', 'LOC', 'ORDINAL', 'EVENT', 'WORK_OF_ART', 'FAC', 'LAW',\n",
    "       'PRODUCT', 'MONEY', 'QUANTITY', 'PERCENT', 'LANGUAGE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.concat([features, pd.DataFrame(X_matrix)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14291, 635)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features[\"Len_sen\"] = len_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0         5\n",
       "1        10\n",
       "2        10\n",
       "3        11\n",
       "4        13\n",
       "5        12\n",
       "6         2\n",
       "7        25\n",
       "8        16\n",
       "9        22\n",
       "10       22\n",
       "11        2\n",
       "12        8\n",
       "13       20\n",
       "14        7\n",
       "15        5\n",
       "16        4\n",
       "17        2\n",
       "18        8\n",
       "19        7\n",
       "20        6\n",
       "21        4\n",
       "22        4\n",
       "23        6\n",
       "24        6\n",
       "25       20\n",
       "26       20\n",
       "27       18\n",
       "28        5\n",
       "29       10\n",
       "         ..\n",
       "14261     8\n",
       "14262     1\n",
       "14263    11\n",
       "14264     8\n",
       "14265     4\n",
       "14266     5\n",
       "14267     8\n",
       "14268     4\n",
       "14269    18\n",
       "14270    16\n",
       "14271     7\n",
       "14272     1\n",
       "14273    12\n",
       "14274     1\n",
       "14275    28\n",
       "14276     7\n",
       "14277     5\n",
       "14278    10\n",
       "14279     3\n",
       "14280     5\n",
       "14281    10\n",
       "14282     9\n",
       "14283    10\n",
       "14284     4\n",
       "14285     1\n",
       "14286     4\n",
       "14287     5\n",
       "14288     6\n",
       "14289     3\n",
       "14290     1\n",
       "Name: Len_sen, Length: 14291, dtype: int64>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features[\"Len_sen\"].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_df = pd.read_csv(\"data_case2_with_affin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14263, 5)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14291, 635)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_article = list(emoji_df['article'])\n",
    "emoji_N_sentence = list(emoji_df['N_sentence'])\n",
    "emoji_df['key'] = [str(emoji_article[i]) + \"_\" + str(emoji_N_sentence[i]) for i in range(emoji_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>article</th>\n",
       "      <th>N_sentence</th>\n",
       "      <th>is_propaganda</th>\n",
       "      <th>affin_score</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US bloggers banned from entering UK\\r\\r\\n</td>\n",
       "      <td>111111112</td>\n",
       "      <td>1</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>111111112_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two prominent US bloggers have been banned fro...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>3</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111111112_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pamela Geller and Robert Spencer co-founded an...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>5</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>111111112_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They were due to speak at an English Defence L...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>7</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>111111112_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A government spokesman said individuals whose ...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>9</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>1.0</td>\n",
       "      <td>111111112_9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences    article  N_sentence  \\\n",
       "0          US bloggers banned from entering UK\\r\\r\\n  111111112           1   \n",
       "1  Two prominent US bloggers have been banned fro...  111111112           3   \n",
       "2  Pamela Geller and Robert Spencer co-founded an...  111111112           5   \n",
       "3  They were due to speak at an English Defence L...  111111112           7   \n",
       "4  A government spokesman said individuals whose ...  111111112           9   \n",
       "\n",
       "    is_propaganda  affin_score          key  \n",
       "0  non-propaganda         -2.0  111111112_1  \n",
       "1  non-propaganda          0.0  111111112_3  \n",
       "2      propaganda         -2.0  111111112_5  \n",
       "3  non-propaganda         -3.0  111111112_7  \n",
       "4  non-propaganda          1.0  111111112_9  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features= all_features.drop(labels = 'text_standard', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = all_features.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_features.iloc[:, 3:].values\n",
    "y = all_features['label'].map({'non-propaganda':0, 'propaganda':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_features['label_dummy'] = all_features['label'].map({'non-propaganda':0, 'propaganda':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10345\n",
       "1     3946\n",
       "Name: label_dummy, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_features['label_dummy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.6/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from sklearn) (0.20.2)\r\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.13.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=0,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Kernel SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2542,  567],\n",
       "       [ 590,  589]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('kernel_svm_with600_iter_with_SMOTE.pkl', 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernal SVM F1 result: 0.504496788009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_s = f1_score(y_test, y_pred)\n",
    "print(\"Kernal SVM F1 result: \" + str(f1_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TextBlob in /opt/conda/lib/python3.6/site-packages (0.15.2)\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/conda/lib/python3.6/site-packages (from TextBlob) (3.4)\n",
      "Requirement already satisfied: singledispatch in /opt/conda/lib/python3.6/site-packages (from nltk>=3.1->TextBlob) (3.4.0.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk>=3.1->TextBlob) (1.12.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.4)\n",
      "Requirement already satisfied: singledispatch in /opt/conda/lib/python3.6/site-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install TextBlob\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TextBlob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-4be6410d7570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TextBlob'"
     ]
    }
   ],
   "source": [
    "from nltk import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
