{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /anaconda3/lib/python3.6/site-packages (0.15.2)\n",
      "Requirement already satisfied: nltk>=3.1 in /anaconda3/lib/python3.6/site-packages (from textblob) (3.2.4)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from nltk>=3.1->textblob) (1.11.0)\n",
      "Requirement already satisfied: textstat in /anaconda3/lib/python3.6/site-packages (0.5.4)\n",
      "Requirement already satisfied: repoze.lru in /anaconda3/lib/python3.6/site-packages (from textstat) (0.7)\n",
      "Requirement already satisfied: pyphen in /anaconda3/lib/python3.6/site-packages (from textstat) (0.9.5)\n",
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.6/site-packages (3.2.4)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied: spacy in /anaconda3/lib/python3.6/site-packages (2.0.18)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.16.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: ujson>=1.35 in /anaconda3/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: regex==2018.01.10 in /anaconda3/lib/python3.6/site-packages (from spacy) (2018.1.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.18.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /anaconda3/lib/python3.6/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /anaconda3/lib/python3.6/site-packages (from spacy) (6.12.1)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /anaconda3/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2017.7.27.1)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.29.1)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /anaconda3/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.8.2)\n",
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /anaconda3/lib/python3.6/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda3/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /anaconda3/lib/python3.6/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!pip install textstat\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import os\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14263"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataprep_task2(path):\n",
    "    \"\"\"Dataprep for Task2 It will return the new data\n",
    "    :param path: Path to the article's taks3 labels file.\n",
    "    Example:\n",
    "    >>> dataprep_task2(\"datasets-v5/tasks-2-3/train/article111111112.task2.labels\")\n",
    "    Note the method will return Pandas DataFrame\n",
    "    \"\"\"\n",
    "    dir_name = os.path.dirname(path)\n",
    "    article_id = os.path.basename(path).split('.')[0]\n",
    "    article_name = os.path.join(dir_name, f'{article_id}.txt')\n",
    "\n",
    "    with open(article_name, 'r') as f:\n",
    "        records = f.readlines()\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['sentences'])\n",
    "\n",
    "    another_df = pd.read_csv(path, sep='\\t', names = ['article', 'N_sentence', 'is_propaganda'])\n",
    "\n",
    "    result_df = pd.concat([df, another_df], axis=1)\n",
    "\n",
    "    return result_df.loc[result_df['sentences'] != '\\n', :]\n",
    "\n",
    "df_task2 = pd.DataFrame([])\n",
    "list_articles = [x for x in os.listdir('/Users/rmania/Downloads/datasets-v5/tasks-2-3/train/') if (x.split('.')[-2] == 'task2' and x.split('.')[-1]=='labels')]\n",
    "for file in list_articles:\n",
    "    df_task2 = df_task2.append(dataprep_task2('/Users/rmania/Downloads/datasets-v5/tasks-2-3/train/' + file))\n",
    "\n",
    "df_task2.to_csv('DataTask2.csv')\n",
    "df = df_task2\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'US bloggers banned from entering UK\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in range(df.shape[0]):\n",
    "    df.loc[article, 'sentiment_polarity'] = TextBlob(df.loc[article, 'sentences']).sentiment[0]\n",
    "    df.loc[article, 'sentiment_subjectivity'] = TextBlob(df.loc[article, 'sentences']).sentiment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#entities tagging\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "df_POS = pd.Series([])\n",
    "for article in range(df.shape[0]):\n",
    "    df_POS = df_POS.append(pd.DataFrame([Counter([x.label_ for x in nlp(df.loc[article, \\\n",
    "                                                            'sentences']).ents])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_readability = pd.DataFrame({'flesch_reading_ease': [textstat.flesch_reading_ease(article) for article in df['sentences']],\n",
    "                 'smog_index': [textstat.smog_index(article) for article in df['sentences']] ,\n",
    "                 'flesch_kincaid_grade': [textstat.flesch_kincaid_grade(article) for article in df['sentences']] ,\n",
    "                 'coleman_liau_index': [textstat.coleman_liau_index(article) for article in df['sentences']] ,\n",
    "                 'automated_readability_index': [textstat.automated_readability_index(article) for article in df['sentences']], \n",
    "                'dale_chall_readability_score': [textstat.dale_chall_readability_score(article) for article in df['sentences']],\n",
    "                'difficult_words': [textstat.difficult_words(article) for article in df['sentences']],\n",
    "              'linsear_write_formula': [textstat.linsear_write_formula(article) for article in df['sentences']],\n",
    "              'gunning_fog': [textstat.gunning_fog(article) for article in df['sentences']],\n",
    "           'text_standard': [textstat.text_standard(article) for article in df['sentences']]})   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_POS.to_csv('/Users/rmania/repos/df_POS_task2.csv')\n",
    "df_readability.to_csv('/Users/rmania/repos/df_readability_task2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Affin scores\n",
    "affin_scores = pd.read_csv('/Users/rmania/repos/data_case2_with_affin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-33b8dd40d68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_with_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_POS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_readability\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    205\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                        copy=copy)\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m                     \u001b[0mobj_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                         \u001b[0mindexers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[1;32m   2885\u001b[0m                         raise ValueError(\"cannot reindex a non-unique index \"\n\u001b[1;32m   2886\u001b[0m                                          \"with a method or limit\")\n\u001b[0;32m-> 2887\u001b[0;31m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2889\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreserve_names\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_indexer_non_unique\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   2706\u001b[0m             \u001b[0mtgt_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m         \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_indexer_non_unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(a, new_shape)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0mextra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNa\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_copies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mextra\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mextra\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_with_features = pd.concat([df[2:], df_POS, df_readability], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_features = pd.concat([df, df_POS, df_readability], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentences</th>\n",
       "      <th>article</th>\n",
       "      <th>N_sentence</th>\n",
       "      <th>is_propaganda</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>US bloggers banned from entering UK\\n</td>\n",
       "      <td>111111112</td>\n",
       "      <td>1</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Two prominent US bloggers have been banned fro...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>3</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Pamela Geller and Robert Spencer co-founded an...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>5</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>They were due to speak at an English Defence L...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>7</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>-0.108333</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>A government spokesman said individuals whose ...</td>\n",
       "      <td>111111112</td>\n",
       "      <td>9</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                          sentences    article  \\\n",
       "0      0              US bloggers banned from entering UK\\n  111111112   \n",
       "1      2  Two prominent US bloggers have been banned fro...  111111112   \n",
       "2      4  Pamela Geller and Robert Spencer co-founded an...  111111112   \n",
       "3      6  They were due to speak at an English Defence L...  111111112   \n",
       "4      8  A government spokesman said individuals whose ...  111111112   \n",
       "\n",
       "   N_sentence   is_propaganda  sentiment_polarity  sentiment_subjectivity  \n",
       "0           1  non-propaganda            0.000000                0.000000  \n",
       "1           3  non-propaganda            0.500000                1.000000  \n",
       "2           5      propaganda            0.000000                0.000000  \n",
       "3           7  non-propaganda           -0.108333                0.125000  \n",
       "4           9  non-propaganda            0.350000                0.333333  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_affine = df.merge(affin_scores, how = 'inner', \\\n",
    "                                        left_on = ['article', 'N_sentence'],\\\n",
    "                                        right_on = ['article', 'N_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'article', 'N_sentence', 'is_propaganda_x',\n",
       "       'sentiment_polarity', 'sentiment_subjectivity', 'sentences_y',\n",
       "       'affin_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_affine.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_affine = df_with_affine.drop(labels = ['sentences_x', 'is_propaganda_y'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_affine.columns = ['index', 'article', 'N_sentence', 'is_propaganda',\n",
    "       'sentiment_polarity', 'sentiment_subjectivity', 'sentences', 'affin_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14263, 8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_affine.head()\n",
    "df_with_affine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feat = pd.concat([df_POS, df_readability], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_with_features = pd.concat([df_with_affine, df_POS, df_readability], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_features= pd.read_csv('/Users/rmania/repos/df_task2_with_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from itertools import chain\n",
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# spaCy\n",
    "import spacy\n",
    "#import spacy_wordnet\n",
    "#from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Configuration reader\n",
    "#sys.path.append(os.path.abspath(\"..\\\\Utils\"))\n",
    "#from config_parser import read_configuration\n",
    "\n",
    "class DataTokenization:\n",
    "\n",
    "    def __init__(self, tickets):\n",
    "        \"\"\"\n",
    "        Init DataTokenization object\n",
    "\n",
    "        Args:\n",
    "            ds: Dataframe (pandas type) having only index and one column with single document per record.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the input meets expectations\n",
    "        if not isinstance(tickets, list):\n",
    "            raise Exception('The input should be of type list')\n",
    "\n",
    "        self.tickets = tickets\n",
    "  #      self.data_parser_conf = read_configuration(\"dataParserService\")\n",
    "      \n",
    "        # define default values for different attributes\n",
    "        # if they were not provided in the configuration file\n",
    "\n",
    "        # tickets_tokenizer args\n",
    "        \n",
    "        self.use_stemming = \"Yes\"\n",
    "\n",
    "        self.n_grams_type = 'simple'\n",
    "    \n",
    "        self.n_grams = \"3\"\n",
    "   \n",
    "        self.stopwords = stopwords.words('english')\n",
    "    \n",
    "        self.use_phrases = \"Yes\"\n",
    "\n",
    "\n",
    "    def _document_tokenizer(self,\n",
    "                            document: str,\n",
    "                            m_stopwords: list,\n",
    "                            wordnet_lemmatizer,\n",
    "                            stemmer,\n",
    "                            synonyms_method):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            document: a document. Could be from single word to multiple lines/paragraphs text.\n",
    "            m_stopwords [Optional]: This is a list of the stopwords which should be excluded from the tokens\n",
    "                                  The default value is empty list which means no words will be excluded.\n",
    "            stemmer: [object] Stemmming object.         \n",
    "            wordnet_lemmatizer: [object] Lemmatizer object.         \n",
    "            synonyms_method [Optional]: How the synonyms are exctacted - using nltk or spacy\n",
    "\n",
    "        Returns:\n",
    "            A list of prepared tokens of the documnet.\n",
    "\n",
    "        Example:\n",
    "            input document:\n",
    "            'John.Doe@examplemail.com cannot log onto the SDE (or reset the password).\n",
    "            Emma raised ART request  for an SDE login for John.Doe@examplemail.com\n",
    "            This request has status \"COMPLETE\", but in the \"COMMENTS\" field there is a message \"Failed to add user Example.name to PORT_T_NEW\"\n",
    "\n",
    "            outputted tokens:\n",
    "            ['log', 'onto', 'sde', 'reset', 'password', 'rais', 'request', 'sde', 'login',\n",
    "            'request', 'status', 'complet', 'comment', 'messag', 'fail', 'add', 'user', 'examplename', 'porttnew']\n",
    "        \"\"\"     \n",
    "        \n",
    "        try:\n",
    "            # remove email adresses\n",
    "         #   document = ' '.join([t for t in document.split() if not any(c == '@' for c in t)]) \n",
    "\n",
    "            # split string into words (tokens)\n",
    "            tokens = tokenize.word_tokenize(document)   \n",
    "\n",
    "            # remove any digits, i.e. \"3rd edition\"; remove any path, i.e. \"//math/lib\" and remove any path on PC\n",
    "            # v1\n",
    "            tokens = [t for t in tokens if not any((c.isdigit() or c == '/' or c == '\\\\') for c in t)] \n",
    "            # v2\n",
    "            # tokens = [re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', t, flags=re.MULTILINE) for t in tokens]\n",
    "\n",
    "            # remove special chars\n",
    "            tokens = [re.sub(r'[^a-zA-Z0-9\\s]', '', t) for t in tokens]  \n",
    "            \n",
    "            # remove short words which don't contain capital letters. \n",
    "            tokens = [t for t in tokens if not (len(t) <= 2 and t != 'no' and t != 'nt' and (t.islower()) or t == '')] \n",
    "            \n",
    "            # downcase\n",
    "            tokens = [t.lower() for t in tokens]  \n",
    "            \n",
    "            # remove stopwords\n",
    "            tokens = [t for t in tokens if t not in m_stopwords]  \n",
    "            \n",
    "            # put words into base form\n",
    "            tokens = [wordnet_lemmatizer.lemmatize(t, 'v') for t in tokens]  \n",
    "\n",
    "            # put words into core form\n",
    "            # execute only if no synonyms are needed\n",
    "            # if both stemming and synonyms should be added - this is added in visual script\n",
    "            if ((self.use_stemming.lower().strip() == 'yes'\n",
    "                or self.use_stemming.lower().strip() == 'y')\n",
    "               and self.use_phrases.lower().strip() != 'yes'\n",
    "                and self.use_phrases.lower().strip() != 'y'):\n",
    "\n",
    "                tokens = [stemmer.stem(t) for t in tokens]  \n",
    "\n",
    "        except Exception as err:\n",
    "            raise Exception(\"There was an issue with tokenizing a document:\" + str(err))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "  \n",
    "\n",
    "    def _phrase_generator(self, \n",
    "                         wordnet_lemmatizer, stemmer):\n",
    "        \"\"\"\n",
    "        This method generates the phrases based on prepared tickets from \n",
    "        _document_tokenizer method.\n",
    "\n",
    "        Args:\n",
    "            wordnet_lemmatizer: [object] Lemmatizer object. \n",
    "\n",
    "         Returns:\n",
    "            A list of prepared phreses of the corpus.        \n",
    "        \"\"\"\n",
    "\n",
    "        corpus_all_phrases = \\\n",
    "            [self._document_tokenizer(self.tickets[doc], m_stopwords = self.stopwords,\n",
    "             wordnet_lemmatizer = wordnet_lemmatizer, stemmer = '', synonyms_method = 'none') \n",
    "             for doc in range(len(self.tickets))]\n",
    "\n",
    "        docs = [' '.join([t for t in doc]) for doc in corpus_all_phrases]\n",
    "\n",
    "        # set spaCy object with wordnet annotator\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "#        nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')\n",
    "        bad_deps = ('prep', 'aux')\n",
    "        phrase_list = []\n",
    "        phrase_list_per_doc = []\n",
    "\n",
    "        for doc in docs:\n",
    "            # take the phases using spaCy object with noun_chunks attribute\n",
    "            phrase_list_per_doc = []\n",
    "            for phrase in nlp(doc).noun_chunks:\n",
    "               \n",
    "                # exclude bad_deps\n",
    "                while len(phrase) > 1 and phrase[0].dep_ not in bad_deps:\n",
    "                    phrase = phrase[1:]\n",
    "\n",
    "                # check if our phrase have one or more word and            \n",
    "                if len(phrase) > 1 and len(str(phrase))>1:\n",
    "                    # Merge the tokens, e.g. good_ideas\n",
    "                    phrase = str(phrase.text).lower().replace(\" \", \"_\")\n",
    "                    phrase_list_per_doc.append(phrase)\n",
    "        \n",
    "                if  len(phrase_list_per_doc) > 0:    \n",
    "                    phrase_list.append(phrase_list_per_doc)\n",
    "                else:\n",
    "                     phrase_list.append(\"None\")\n",
    "        return phrase_list\n",
    "\n",
    "\n",
    "    def tickets_tokenizer(self):\n",
    "       \n",
    "        # load object from the related packages - Lemmatizer\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # load object from the related packages - Stemming\n",
    "        if (self.use_stemming.lower().strip() == 'yes'\n",
    "         or self.use_stemming.lower().strip() == 'y'):\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "        else:\n",
    "            stemmer = ''\n",
    "\n",
    "        # Perform the tokenization\n",
    "        corpus_all_tokens = \\\n",
    "            [self._document_tokenizer(self.tickets[doc], m_stopwords = self.stopwords,\n",
    "             wordnet_lemmatizer = wordnet_lemmatizer, stemmer = stemmer, synonyms_method = '') \n",
    "             for doc in range(len(self.tickets))]\n",
    "\n",
    "        corpus_all_tokens = [' '.join(doc) for doc in corpus_all_tokens]\n",
    "        corpus_all_tokens = [doc if str(doc).strip() != '' else 'test' for doc in corpus_all_tokens]\n",
    "\n",
    "        # Create a list of list per token for bi-grams\n",
    "        n_grams_2 = []\n",
    "        n_grams_3 = []\n",
    "\n",
    "        if (int(self.n_grams) != 0 and\n",
    "            int(self.n_grams) != 2 and \n",
    "            int(self.n_grams) != 3):\n",
    "            raise Exception(\"You need to enter valid value for n-grams: 0, 2 or 3 as  integer value.s \\n You have used {self.n_grams} !\")        \n",
    "\n",
    "        if (int(self.n_grams) == 2 or int(self.n_grams) == 3):\n",
    "#pdb.set_trace()\n",
    "            if (self.n_grams_type.lower().strip() == 'simple'):\n",
    "                n_grams_2 = [[a + \"_\" + b for a,b in ngrams(tokens.split(), 2)] for tokens in corpus_all_tokens]  \n",
    "                if (int(self.n_grams) == 3): \n",
    "                    n_grams_3 = [[a + \"_\" + b + \"_\" + c for a,b,c in ngrams(tokens.split(), 3)] for tokens in corpus_all_tokens]\n",
    "            \n",
    "        n_grams_2 = [' '.join(doc) for doc in n_grams_2]      \n",
    "        n_grams_3 = [' '.join(doc) for doc in n_grams_3]\n",
    "        # Phrases \n",
    "        if (self.use_stemming.lower().strip() == 'yes'\n",
    "            or self.use_stemming.lower().strip() == 'y'):\n",
    "\n",
    "            try:\n",
    "                phrase_list = list(self._phrase_generator(wordnet_lemmatizer = wordnet_lemmatizer, stemmer = stemmer))\n",
    "                phrase_list = [' '.join(set_phrases) if set_phrases != \"None\" else set_phrases for set_phrases in phrase_list]\n",
    "            except Exception as err:\n",
    "                raise Exception(\"There was an issue with separatig the key phreses:\" + str(err))\n",
    "\n",
    "        return (corpus_all_tokens, n_grams_2, n_grams_3, phrase_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:220: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# load the service params from configuration file\n",
    "#    sys.path.append(os.path.abspath(\"..\\\\Utils\"))\n",
    "#   from config_parser import read_configuration\n",
    "\n",
    "# load the testing script\n",
    "# this scripts produce examples\n",
    "# sys.path.append(os.path.abspath(\"..\\\\Testting\\\\Scripts\"))\n",
    "#from testting_data_parser import TestDataParser\n",
    "\n",
    "# barplot for most common phrases\n",
    "# sys.path.append(os.path.abspath(\"..\\\\Utils\"))\n",
    "#from word_freqs import word_frequency_barplot\n",
    "\n",
    "\n",
    "# Load the tickets data\n",
    "data = df\n",
    "#   data = pd.read_csv(\"..\\\\Resources\\\\self_service_four_categories.csv\", encoding='ISO-8859-1', index_col = 0)\n",
    "\n",
    "# take the text unstructured part of the dataset\n",
    "# this df format with the index from the original dataset\n",
    "documents = list(data['sentences'])\n",
    "# load the DataPreprocessing object - the main DataParser object (by now)\n",
    "data_parser_object = DataTokenization(documents)\n",
    "\n",
    "# clock the operation\n",
    "tic = time.time()\n",
    "\n",
    "corpus_all_tokens, n_grams_2, n_grams_3, phrase_list = data_parser_object.tickets_tokenizer()\n",
    "\n",
    "#pdb.set_trace()\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "time_diff = toc - tic\n",
    "\n",
    "\n",
    "\n",
    "# Create barplot of most common phrases\n",
    "\n",
    "#    phrase_list_v2 = [phrase for doc in phrase_list for phrase in doc.split() if phrase not in \"None\"]\n",
    "\n",
    "#    word_frequency_barplot(tokenized_text = phrase_list_v2, savedir = \"..\\\\Resources\")         \n",
    "\n",
    "\n",
    "corpus_all_tokens_df = pd.DataFrame(corpus_all_tokens, columns=['corpus_all_tokens'])\n",
    "\n",
    "corpus_all_tokens_df.to_csv(\"corpus_all_tokens_df2.csv\")\n",
    "\n",
    "n_grams_2_df = pd.DataFrame(n_grams_2, columns=['n_grams_2'])\n",
    "\n",
    "n_grams_2_df.to_csv(\"n_grams_2_df2.csv\")\n",
    "\n",
    "n_grams_3_df = pd.DataFrame(n_grams_3, columns=['n_grams_3'])\n",
    "\n",
    "n_grams_3_df.to_csv(\"n_grams_3_df2.csv\")\n",
    "\n",
    "phrase_list_df= pd.DataFrame(phrase_list, columns=['phrase_list'])\n",
    "\n",
    "phrase_list_df.to_csv(\"phrase_list2.csv\")\n",
    "\n",
    "combined_dataset = pd.concat([corpus_all_tokens_df, n_grams_2_df, n_grams_3_df], axis=1, join = 'inner')\n",
    "\n",
    "combined_dataset.to_csv(\"corpus_all_tokens_n_grams_df2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('corpus_all_tokens_df2.csv')\n",
    "ngrams_2 = pd.read_csv('n_grams_2_df2.csv')\n",
    "corpus_all_tokens_n_grams_df2 = pd.read_csv('corpus_all_tokens_n_grams_df2.csv')\n",
    "phrase_list2_df = pd.read_csv(\"phrase_list2.csv\")\n",
    "total_df = corpus_all_tokens_n_grams_df2.join(phrase_list2_df.phrase_list)\n",
    "corpus_all_tokens = total_df[['corpus_all_tokens']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "total_df['fulltext'] = pd.Series([])\n",
    "for i in range(total_df.shape[0]):\n",
    "    if total_df.phrase_list[i].strip() != \"None\":\n",
    "        total_df['fulltext'][i] = str(total_df.corpus_all_tokens[i]) + \" \" + \\\n",
    "        str(total_df.n_grams_2[i]) + \" \" + str(total_df.n_grams_3[i]) + \" \" + str(total_df.phrase_list[i])\n",
    "    else:\n",
    "        total_df['fulltext'][i] = str(total_df.corpus_all_tokens[i]) + \" \" + \\\n",
    "        str(total_df.n_grams_2[i]) + \" \" + str(total_df.n_grams_3[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_sentence = [len(doc.split()) for doc in corpus_all_tokens_n_grams_df2['corpus_all_tokens'].values]\n",
    "total_df['sentence_length'] = len_sentence\n",
    "total_df.to_csv('/Users/rmania/repos/total_df_task2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tokens = list(total_df['fulltext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus_all_tokens_unique = set([w for doc in all_tokens for w in doc.split()])\n",
    "print(\"Total number of uniqure words in the corpus: \" + str(len(corpus_all_tokens_unique)))\n",
    "\n",
    "# encode the words in the dictionary in order to create keys DB table \n",
    "word2idx = {w:i for i,w in enumerate(corpus_all_tokens_unique)}\n",
    "tfidf_object = TfidfVectorizer(decode_error='ignore', vocabulary = word2idx)\n",
    "X_tfidf = tfidf_object.fit_transform(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame(X_tfidf.toarray())\n",
    "test.to_csv(\"X_tfidf.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=600, n_iter=2, random_state = 1234)\n",
    "X_matrix = lsa.fit_transform(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_matrix_df = pd.DataFrame(X_matrix)\n",
    "X_matrix_df.to_csv('X_matrix_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = df_with_features[['sentence_id', 'article_id',\n",
    "       'label', 'flesch_reading_ease', 'smog_index',\n",
    "       'flesch_kincaid_grade', 'coleman_liau_index',\n",
    "       'automated_readability_index', 'dale_chall_readability_score',\n",
    "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
    "       'text_standard', 'sentiment_polarity', 'sentiment_subjectivity',\n",
    "       'Unnamed: 0.2', '0', 'GPE', 'CARDINAL', 'ORG', 'NORP', 'PERSON', 'DATE',\n",
    "       'TIME', 'LOC', 'ORDINAL', 'EVENT', 'WORK_OF_ART', 'FAC', 'LAW',\n",
    "       'PRODUCT', 'MONEY', 'QUANTITY', 'PERCENT', 'LANGUAGE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features = pd.concat([features, pd.DataFrame(X_matrix)], axis = 1)\n",
    "all_features[\"Len_sen\"] = len_sentence\n",
    "all_features= all_features.drop(labels = 'text_standard', axis = 1)\n",
    "all_features = all_features.fillna(value = 0)\n",
    "\n",
    "all_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = all_features.iloc[:, 3:].values\n",
    "y = all_features['label'].map({'non-propaganda':0, 'propaganda':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
